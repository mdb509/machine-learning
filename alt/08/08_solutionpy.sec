import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

np.random.seed(1)

iris = datasets.load_iris()
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)


def bootstrap(X: np.array, y: np.array, target_size: int) -> tuple:
    """
    Implement bootsrapping without using sklearn!

    Args:
        X (ndarray): The data in shape NxM
        y (ndarray): The target in shape N
        target_size (int): The size of the sample to return
    
    Returns:
        tuple(ndarray, ndarray): The bootstrapped sample where
            the data is at the first postion and the target at
            the second.
    """
    assert X.shape[0] == len(y)

    indices = np.random.randint(0, high=X.shape[0], size=target_size)
    return X[indices], y[indices]


class RandomForestClassifier:

    def __init__(self, num_trees: int = 100, bootstrapping_size: int = 50, feature_subset: int = 3):
        """
        Initializes the RandomForestClassifier.

        Args:
            num_trees (int): Number of weak learner trees.
            bootstrapping_size (int): The size of the bootstrapped samples.
            feature_subset (int): The number of features to use for the
                splitting criteria.
        """
        self.num_trees = num_trees
        self.bootstrapping_size = bootstrapping_size
        self.feature_subset = feature_subset
        self.trees = []

    def fit(self, X: np.array, y: np.array):
        """
        Fitting the random forest to the data given.

        Args:
            X (ndarray): The data in shape NxM.
            y (ndarray): The target in shape N.
        """
        assert X.ndim == 2 and y.ndim == 1, "Wrong dimensions for X!"
        assert X.shape[0] == len(y), "Size of X and y does not match!"

        # TODO: Implement the algorithm to learn a random forrest.
        # you may use sklearn.tree.DecisionThreeClassifier as the
        # implementation for the weak learner

        for b in range(self.num_trees):
            x_sample, y_sample = bootstrap(X, y, self.bootstrapping_size)

            tree = DecisionTreeClassifier(
                max_depth=1,
                max_features=self.feature_subset,
                random_state=1,
            )

            tree.fit(x_sample, y_sample)
            self.trees.append(tree)

    def predict(self, X: np.array) -> np.array:
        """
        Predict the target class for the data X using majority voting.

        Args:
            X (ndarray): The data in shape NxM.
        
        Returns:
            (ndarray): The predictions in shape N
        """
        assert X.ndim == 2
        votes = np.array([t.predict(X) for t in self.trees]).T
        predictions = []
        for row in votes:
            values, counts = np.unique(row, return_counts=True)
            predictions.append(values[np.argmax(counts)])
        return np.array(predictions)


rf = RandomForestClassifier()

rf.fit(X_train, y_train)
y_hat = rf.predict(X_test)
print(accuracy_score(y_test, y_hat))

# Varying bootstrapping_size and calculating accuracies
bootstrapping_sizes = np.linspace(1, len(X_train), dtype=int)
accuracies = []

for size in bootstrapping_sizes:
    rf = RandomForestClassifier(bootstrapping_size=size)
    rf.fit(X_train, y_train)
    y_hat = rf.predict(X_test)
    accuracies.append(accuracy_score(y_test, y_hat))

# Plotting the results
plt.figure(figsize=(10, 6))
plt.plot(bootstrapping_sizes, accuracies)
plt.xlabel('Bootstrapping Size')
plt.ylabel('Accuracy')
plt.title('Random Forest Classifier Accuracy vs Bootstrapping Size')
plt.grid(True)
plt.show()
